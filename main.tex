\documentclass[a4paper,14pt]{extreport}
\usepackage[utf8]{inputenc} % Set Encoding
\usepackage[T1,T2A]{fontenc}
\usepackage{fontspec} % Using custom fonts (requires -xelatex flag)
\usepackage[russian,english,ukrainian]{babel} % Using languages
\usepackage{geometry} % Set margins
\usepackage[raggedright]{titlesec} % For section modification
\usepackage{indentfirst} % Inserts indents in paragraphs
\usepackage{minted} % For code listing
\usepackage{graphicx} % For image insertions
\usepackage{float} % For positioning
\usepackage[section]{placeins}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage{enumitem} % List indents
\usepackage{chngcntr}
\usepackage[english,russian,ukrainian]{babel}
\usepackage[nottoc]{tocbibind}
\usepackage{color}
\usepackage{xpatch}
\definecolor{spot}{rgb}{0,0,0}
\usepackage[colorlinks,allcolors=spot,bookmarksopen=true,pdfstartview=FitH]{hyperref}
\usepackage{amsmath}

\counterwithout{section}{chapter}

\pagestyle{fancy}

\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0pt}
\fancyheadoffset{0mm}
\fancyfootoffset{0mm}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{17.0pt}

\fancypagestyle{plain}{
    \fancyhf{}
    \rhead{\thepage}}


\pagenumbering{gobble}

\usemintedstyle{bw}

\geometry{
  a4paper,
  left=30mm,
  right=20mm,
  top=20mm,
  bottom=20mm
}

\DeclareCaptionLabelFormat{gostfigure}{Рисунок #2}
\DeclareCaptionLabelFormat{gosttable}{Таблиця #2}
\DeclareCaptionLabelSeparator{gost}{~---~}
\captionsetup{labelsep=gost}
\captionsetup[figure]{labelformat=gostfigure, justification=centering,
labelsep=gost}
\captionsetup[table]{labelformat=gosttable, labelsep=gost}

\setlength\parindent{2.5em}
\renewcommand{\baselinestretch}{2.0}
\linespread{1.3} % Set default line spacing

\setmainfont{Liberation Serif} % Set default font on Linux

\titleformat{\section}
{\normalfont}{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont}{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont}{\thesubsubsection}{1em}{}

\titleformat{\chapter}[block]
    {\filcenter\bfseries}
    {\thechapter}
    {1em}
    {\MakeUppercase}{}

\renewcommand{\headrulewidth}{0pt}

\titlespacing{\chapter}{0pt}{-30pt}{2em}
\titlespacing\section{0cm}{2em}{1ex}
\titlespacing\subsection{0cm}{1ex}{1ex}

\newcommand\chap[1]{%
  \chapter*{#1}%
  \addcontentsline{toc}{chapter}{\uppercase{#1}}}

\graphicspath{{./pics}}

\setlist[enumerate]{leftmargin=4em}

\begin{document}
\begin{titlepage}
	\centering
    Міністерство освіти і науки України
    
    Харківський національний університет радіоелектроніки
    \vspace{1cm}

    Факультет комп'ютерних наук
    \vspace{1cm}

    Кафедра штучного інтелекту

    \vspace{2cm}
    \uppercase{Реферат}
    \vspace{1cm}

    на тему <<Бот Dvango>>
    \vspace{1cm}

    з дисципліни <<Системи інтелектуальної обробки природно-мовної інформації>>

    \begin{flushleft}
    \vspace{4cm}
    \begin{minipage}[t]{8cm}
        Виконали ст. гр. ІТШІ-18-1:

        Соколенко Дмитро Олександрович

        Апраксін Антон Романович
    \end{minipage}
    \hfill
    \begin{minipage}[t]{6cm}
        Прийняв:\\
        проф. каф. ШІ Рябова Н. В.\\
        з оцінкою ``\rule{2cm}{0.15mm}''\\
        ``\rule{0.7cm}{0.15mm}''\rule{2cm}{0.15mm}20\rule{0.7cm}{0.15mm}р
    \end{minipage}
    \end{flushleft}
	\vfill

	{Харків \the\year{}}
\end{titlepage}
\pagenumbering{arabic}
\setcounter{page}{3}
am
\chap{Реферат}


\newpage
\tableofcontents
\newpage


\chap{Вступ}


\chap{Основна частина}
\section{Аналіз предметної галузі}
\subsection{Штучні нейронні мережі}
Штучна нейронна мережа побудована навколо біологічної метафори.
Будова первинної зорової кори відносно добре відома,
і вчені виграли Нобелівську премію з фізіології за відкриття в
1962 р. організації нейронів у перших кортикальних
шарах \cite{cortex-stuff}. Таким чином, в надзвичайно
спрощеному вигляді
мозку нейрони організовані шарами, кожен нейрон отримує
інформацію з попереднього шару, виконує дуже просте
обчислення і передає свій результат нейронам наступного
шару. Однак слід пам’ятати, що це лише метафора та джерело
натхнення: біологічні мережі мають набагато складніші
зв’язки, а математичні рівняння, що керують ними, також
є більш складними \cite{cortex-equations}.

\subsubsection{Персептрон}
Перцептрони -- винайдені Френком Розенблаттом у 1957 році
\cite{rozenblatt}, є найпростішою
нейронною мережею, яка складається з $n$ кількості входів, лише одного
нейрона та одного виходу, де $n$ - кількість рис нашого набору
даних. Процес вираховухання результату класифікації починається
з обчислення зваженої суми
по формулі \ref{eq:perceptron1}.

\begin{equation}
    z = \sum^n_{i=1} x_i w_i + b
    \label{eq:perceptron1}
\end{equation}

де $w$ -- вектор дійсних ваг, $b$ -- зміщення.
Коефіцієнт зміщення зміщує межу прийняття рішення
від початку координат і не залежить від будь-якого
вхідного значення.

Потім на зваженій сумі використовується функція активації.

\begin{equation}
    \phi(z) = \begin{cases}
        0 ,& z \le w_0 \\
        1 ,& z > w_0 \\
    \end{cases}
    \label{eq:perceptron2}
\end{equation}

\subsubsection{Багатошаровий персептрон}
Перші персептрони містили лише один шар. Такі одношарові
архітектури занадто прості, щоб вони могли виконувати складні задачі,
і завдяки додаванню декількох шарів ми можемо розрахувати більш складні
функції. Таким чином, глибокі нейронні мережі використовують дуже
велику кількість шарів. За останні роки ці архітектури дозволили
отримати дуже вражаючі результати для розпізнавання зображень та
відео, а також для автоматичного перекладу текстів.

Багатошаровий персептрон добавляє концепцію
``прихованого шару''. Тут нейрони розташовані у наборі шарів,
і кожен шар містить деяку кількість однакових нейронів.
Кожен нейрон в одному шарі з'єднаний з кожним нейроном в наступному шарі;
ми говоримо, що мережа тісно пов’язана. Перший рівень - це вхідний шар,
і його нейрони приймають значення вхідних рис. 
Останній рівень є вихідним шаром, і він має один нейрон для
кожного значення, яке виводить мережа (тобто один нейрон
в разі регресії або двійкової
класифікації, або $K$  у випадку класифікації з $K$ класами).
Усі шари між ними відомі як приховані шари, тому що ми не знаємо
заздалегідь, що ці одиниці повинні обчислювати, і це потрібно
виявити під час навчання \cite{nn:multilayer-perceptrons}.

Обчислення, які робить багатошарова мережа з двома
прихованими шарами можна
представити у векторизованій формі так \cite{nn:multilayer-perceptrons}:

\begin{align}
    h^{(1)} &= \phi^{(1)}(W^{(1)}x + b^{(1)}) \nonumber\\
    h^{(2)} &= \phi^{(1)}(W^{(1)}h^{(1)} + b^{(2)})\\
    y &= \phi^{(1)}(W^{(1)}h^{(2)} + b^{(3)}) \nonumber
\end{align}

Векторизована форма зазвичай використовується так як підсумовування
та індекси можуть бути громіздкими. Оскільки кожен шар містить
багато нейронів, ми представляємо активації всіх його нейронів
за допомогою вектора активації $h^{(l)}$. Оскільки для кожної пари
нейронів у двох послідовних шарах існує вага, можна представити
ваги кожного шару за допомогою вагової матриці $W^{(l)}$. Кожен шар також має вектор зміщення $b^{(l)}$.

\subsubsection{Навчання мереж}
Навчання нейронної мережі полягає у виборі ``найкращих'' можливих
ваг набору нейронів, що складають мережу. Таким чином,
необхідно вибрати значення цих ваг, щоб найкраще вирішити
завдання, що вивчаються за набором навчальних даних.

Процедура навчання, таким чином, полягає у модифікації ваг $w$,
таких, що для кожного $x$, мережа $f_w$ передбачає якомога
точніше їх асоціювання, тобто в кінці тренування $y \approx f_w(x)$.
Простий вибір -- мінімізувати суму $E(w)$ квадратів
помилок, які математично записують як

\begin{equation}
    \min_w E(w) = \sum (f_w(x) - y)^2
\end{equation}

Це відповідає задачі оптимізації, оскільки необхідно знайти
набір параметрів, який оптимізує певну ціль.
Це складна проблема, оскільки параметрів дуже багато, і ці
параметри, особливо параметри прихованих шарів, мають складний
вплив на результат. На щастя, існують ефективні математичні та
алгоритмічні методи для ефективного вирішення цього типу задач
оптимізації. Вони ще не повністю зрозумілі на теоретичному
рівні, і це дуже активна область досліджень. Ці методи
оптимізації модифікують ваги мережі, щоб покращити її та
зменшити помилку навчання $E(w)$. Математичне
правило для прийняття рішення щодо стратегії оновлення
ваги називається зворотним розповсюдженням \cite{nn:backpropagation}.

У методі зворотного розповсюдження помилки
використовується правило складної похідної.
Часткові похідні функції помилки відносно різних ваг та зміщень
зворотньо поширюються через багатошаровий персептрон.
Цей акт диференціації дає нам градієнт, або ландшафт помилок,
який дозволяє приблизитися до мінімуму помилки. Це робиться
за допомогою алгоритмів оптимізації на основі градієнта, таких
як стохастичний градієнтний спуск, Adam, Monentum та інші
\cite{gradient-descend}.

\subsection{Рекурентні нейронні мережі}


\subsection{Механізм уваги}
Увага до певної міри мотивована тим, як ми приділяємо зорову увагу
різним регіонам зображення або співвідносимо слова в одному реченні.

Зорова увага людини дозволяє зосередитись на певному регіоні з
``високою роздільною здатністю'', одночасно сприймаючи навколишнє
зображення в ``низькій роздільній здатності'', а потім відрегувати
фокусну точку або зробити відповідний висновок.
Враховуючи невеликий фрагмент зображення, пікселі в решті надають підказки, що
там відображається. Ми очікуємо побачити загострене вухо в жовтій коробці,
тому що ми бачили ніс собаки, ще одне загострене вухо праворуч і очі
Шіби (речі в червоних квадратах) (Рис. \ref{fig:shiba}). Однак
светр та ковдра внизу були
б не такими корисними, як ті собачі риси.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{shiba-example-attention.png}
    \caption{Шіба-іну в одязі}
    \label{fig:shiba}
\end{figure}

Подібним чином ми можемо пояснити зв'язок між словами в одному реченні
або близькому контексті. Коли ми бачимо ``їсти'', ми сподіваємось дуже
скоро зустріти слово, що означає вид їжи.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{sentence-example-attention.png}
    \caption{Одне слово ``поділяє увагу'' іншим словам по-різному}
    \label{fig:attend-example}
\end{figure}

Увагу при глибокому навчанні можна широко трактувати як вектор вагових
коефіцієнтів: для того, щоб передбачити або зробити висновок про один елемент,
наприклад, піксель на зображенні або слово в реченні, ми оцінюємо,
використовуючи вектор уваги, наскільки сильно він співвідноситься з іншими
елементами,
і приймає суму їх значень, зважену вектором уваги, як наближення
цілі \cite{attention}.

Архітектури, засновані на трансформаторах, які в основному
використовуються для моделювання завдань на розуміння мови,
уникають використання рекурентності у нейронних мережах і замість
цього цілком довіряють механізмам самоуваги для побудови
глобальних залежностей між входами та виходами.

\section{Теоретичні дослідження}


\section{Експериментальні дослідження}


\newpage
\include{references.tex}

\newpage
\chap{Додаток 1}
% \centerline{\uppercase{\bf{Програмна реалізація}}}
%     \inputminted[breaklines,linenos=true]{python}{../../src/app.py}

% \chap{Додаток 2}
% \centerline{\uppercase{\bf{Результати роботи}}}
%     \begin{figure}[H]
%         \centering
%         \includegraphics[width=0.75\textwidth]{f1.png}
%     \end{figure}
    
%     \begin{figure}[H]
%         \centering
%         \includegraphics[width=0.75\textwidth]{f2.png}
%     \end{figure}

%     \begin{figure}[H]
%         \centering
%         \includegraphics[width=0.75\textwidth]{f3.png}
%     \end{figure}
\end{document}


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.75\textwidth]{rplot_iris.png}
%     \caption{Графік дерева вибірки Iris з дискретизацією через IG}
%     \label{fig:plot3}
% \end{figure}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{ |c|c|c|c| } 
%      \hline
%      Мова   & \multicolumn{3}{c|}{Метод та датасет} \\ \hline
%             & Iris Thr. & Iris IG & SUSY \\ \hline
%      Python & 0.9333    & 0.9333  & 0.6799 \\ \hline
%      R      & 0.9333    & 0.9777  & 0.6594 \\ 
%      \hline
%     \end{tabular}
%     \caption{Порівняння точності класифікації}
%     \label{tab:t1}
% \end{table}
% \inputminted[breaklines,linenos=true]{scilab}{repl235.txt}
