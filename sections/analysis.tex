\chapter[Аналіз предметної галузі]{АНАЛІЗ ПРЕДМЕТНОЇ ГАЛУЗІ}
% https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf
Глибокі нейронні мережі (ГНМ) -- надзвичайно потужні моделі машинного навчання,
які досягають чудової точності у таких складних задачах, як
розпізнавання мови та візуальне розпізнавання об’єктів.
ГНМ є потужними, оскільки вони можуть виконувати
довільні паралельні обчислення за невелику кількість кроків.
Отже, хоча нейронні мережі пов’язані
зі звичайними статистичними моделями, вони вивчають складні
обчислення. Більше того, великі ГНМ можна навчати зі
зворотним розповсюдженням, коли мічений навчальний набір має
достатньо інформації для визначення параметрів мережі. Таким
чином, якщо існує параметр великого ГНМ, який досягає хороших
результатів (наприклад, оскільки люди можуть вирішити завдання
дуже швидко), зворотне розповсюдження знайде ці
параметри і вирішить проблему.

\section{Штучні нейронні мережі}
Штучна нейронна мережа побудована навколо біологічної метафори.
Будова первинної зорової кори відносно добре відома,
і вчені виграли Нобелівську премію з фізіології за відкриття в
1962 р. організації нейронів у перших кортикальних
шарах \cite{cortex-stuff}. Таким чином, в надзвичайно
спрощеному вигляді
мозку нейрони організовані шарами, кожен нейрон отримує
інформацію з попереднього шару, виконує дуже просте
обчислення і передає свій результат нейронам наступного
шару. Однак слід пам’ятати, що це лише метафора та джерело
натхнення: біологічні мережі мають набагато складніші
зв’язки, а математичні рівняння, що керують ними, також
є більш складними \cite{cortex-equations}.

\subsection{Перцептрон}
Перцептрони -- винайдені Френком Розенблаттом у 1957 році
\cite{rozenblatt}, є найпростішою
нейронною мережею, яка складається з $n$ кількості входів, лише одного
нейрона та одного виходу, де $n$ - кількість рис нашого набору
даних. Процес вираховухання результату класифікації починається
з обчислення зваженої суми
по формулі \ref{eq:perceptron1} \cite{nn:peyre}.

\begin{equation}
    z = \sum^n_{i=1} x_i w_i + b
    \label{eq:perceptron1}
\end{equation}

де $w$ -- вектор дійсних ваг, $b$ -- зміщення.
Коефіцієнт зміщення зміщує межу прийняття рішення
від початку координат і не залежить від будь-якого
вхідного значення.

Потім на зваженій сумі використовується функція активації.

\begin{equation}
    \phi(z) = \begin{cases}
        0 ,& z \le w_0 \\
        1 ,& z > w_0 \\
    \end{cases}
    \label{eq:perceptron2}
\end{equation}

\subsection{Багатошаровий персептрон}
Перші персептрони містили лише один шар. Такі одношарові
архітектури занадто прості, щоб вони могли виконувати складні задачі,
і завдяки додаванню декількох шарів ми можемо розрахувати більш складні
функції. Таким чином, глибокі нейронні мережі використовують дуже
велику кількість шарів. За останні роки ці архітектури дозволили
отримати дуже вражаючі результати для розпізнавання зображень та
відео, а також для автоматичного перекладу текстів.

Багатошаровий персептрон добавляє концепцію
``прихованого шару''. Тут нейрони розташовані у наборі шарів,
і кожен шар містить деяку кількість однакових нейронів.
Кожен нейрон в одному шарі з'єднаний з кожним нейроном в наступному шарі;
ми говоримо, що мережа тісно пов’язана. Перший рівень - це вхідний шар,
і його нейрони приймають значення вхідних рис. 
Останній рівень є вихідним шаром, і він має один нейрон для
кожного значення, яке виводить мережа (тобто один нейрон
в разі регресії або двійкової
класифікації, або $K$  у випадку класифікації з $K$ класами).
Усі шари між ними відомі як приховані шари, тому що ми не знаємо
заздалегідь, що ці одиниці повинні обчислювати, і це потрібно
виявити під час навчання \cite{nn:multilayer-perceptrons}.

Обчислення, які робить багатошарова мережа з двома
прихованими шарами можна
представити у векторизованій формі так \cite{nn:multilayer-perceptrons}:

\vspace{0.5em}
\begin{gather}
\begin{aligned}
    h^{(1)} &= \phi^{(1)}(W^{(1)}x + b^{(1)}) \\
    h^{(2)} &= \phi^{(1)}(W^{(1)}h^{(1)} + b^{(2)}) \\
    y &= \phi^{(1)}(W^{(1)}h^{(2)} + b^{(3)}) 
\end{aligned}
\end{gather}
\vspace{\baselineskip}

Векторизована форма зазвичай використовується так як підсумовування
та індекси можуть бути громіздкими. Оскільки кожен шар містить
багато нейронів, ми представляємо активації всіх його нейронів
за допомогою вектора активації $h^{(l)}$. Оскільки для кожної пари
нейронів у двох послідовних шарах існує вага, можна представити
ваги кожного шару за допомогою вагової матриці $W^{(l)}$. Кожен шар також має вектор зміщення $b^{(l)}$.

\subsection{Навчання мереж}
Навчання нейронної мережі полягає у виборі ``найкращих'' можливих
ваг набору нейронів, що складають мережу. Таким чином,
необхідно вибрати значення цих ваг, щоб найкраще вирішити
завдання, що вивчаються за набором навчальних даних.

Процедура навчання, таким чином, полягає у модифікації ваг $w$,
таких, що для кожного $x$, мережа $f_w$ передбачає якомога
точніше їх асоціювання, тобто в кінці тренування $y \approx f_w(x)$.
Простий вибір -- мінімізувати суму $E(w)$ квадратів
помилок, які математично записують як

\begin{equation}
    \min_w E(w) = \sum (f_w(x) - y)^2
\end{equation}

Це відповідає задачі оптимізації, оскільки необхідно знайти
набір параметрів, який оптимізує певну ціль.
Це складна проблема, оскільки параметрів дуже багато, і ці
параметри, особливо параметри прихованих шарів, мають складний
вплив на результат. На щастя, існують ефективні математичні та
алгоритмічні методи для ефективного вирішення цього типу задач
оптимізації. Вони ще не повністю зрозумілі на теоретичному
рівні, і це дуже активна область досліджень. Ці методи
оптимізації модифікують ваги мережі, щоб покращити її та
зменшити помилку навчання $E(w)$. Математичне
правило для прийняття рішення щодо стратегії оновлення
ваги називається зворотним розповсюдженням \cite{nn:backpropagation}.

У методі зворотного розповсюдження помилки
використовується правило складної похідної.
Часткові похідні функції помилки відносно різних ваг та зміщень
зворотньо поширюються через багатошаровий персептрон.
Цей акт диференціації дає нам градієнт, або ландшафт помилок,
який дозволяє приблизитися до мінімуму помилки. Це робиться
за допомогою алгоритмів оптимізації на основі градієнта, таких
як стохастичний градієнтний спуск, Adam, Monentum та інші
\cite{gradient-descend}.

% Всі інші моделі нейронних мереж будуються тих самих принципах,
% на яких працюють
% багатошарові перцептрони.

\section{Рекурентні нейронні мережі}
% Для того щоб 

На відміну від звичайних перцептронів, рекурентні нейронні
мережі (РНМ)
беруть на вхід не лише значення поточного вхідного вектору,
але й те, що вони сприйняли раніше в часі.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{recurrent-nn.png}
    \caption{Ілюстрація структури рекурентної нейронної мережі}
    \label{fig:rnn}
\end{figure}

Рішення рекурентної мережі, досягнуте на етапі часу $t-1$, впливає
на рішення, яке вона прийме через один шаг на етапі часу $t$.
Отже, рекурентні мережі мають два джерела вхідних даних -- поточні
та недавнє минуле, які разом визначають, як мережа
реагує на нові дані.

Можна сказати, що рекурентні зв'язки в нейронній мережі
дають можливість пам'ятати попередні рішення.
Додавання пам’яті до нейронних мереж має ціль: у самій послідовності
є інформація, і рекуррентні мережі використовують її для виконання
завдань, які мережі прямого зв'язку не можуть.
А саме, прихований стан дозволяє шукати
кореляційні залежності між подіями, розділеними багатьма моментами,
і ці кореляції називаються ``довгостроковими залежностями'',
оскільки подія за течією часу залежить і є функцією однієї
або кількох подій, що відбулися раніше.

Звичайна рекурентна одиниця описується формулами:

\vspace{0.5em}
\begin{gather}
\begin{aligned}
    h_t &= \phi(W_h h_{t-1}+W_x x_t+b)\\
    y_t &= h_t
\end{aligned}
\end{gather}
\vspace{\baselineskip}

Тут $x_t$ це вхід, $h_t$ -- рекурентна інформація, а $y_t$ -- 
результат на кроці $t$.

Однак рекурентні мережі, що складаються зі стандартних рекурентних
одиниць, не здатні обробляти довгострокові залежності: у міру
зростання розриву між відповідними входами важко
вивчити з'єднану інформацію. Основними причинами проблеми
довгострокових залежностей є те, що сигнали про помилки,
що надходять назад у часі, мають тенденцію або підірватися,
або зникнути \cite{nn:recurrent-hard}.

Для вирішення цієї проблеми було розроблено багато
методів, таких як LSTM \cite{nn:lstm}, GRU \cite{nn:gru}. Вони
гарно себе показали у задачах, які потребують захоплення
довгострокових залежностей. Такі задачі включають, але не обмежуються,
розпізнавання мови, машинний переклад.

Задачі, де завдання полягає у відображенні однієї послідовності в
іншу (машинний переклад, розпізнавання мови) становлять
виклик для рекурентних
нейронних мереж, оскільки вони вимагають, щоб розмірність
входів і виходів була відомою та фіксованою.
Для вирішення цієї проблеми була запропонована
модель ``seq2seq'' \cite{nn:seq2seq}. Загалом, вона спрямована
на перетворення вхідної послідовності (джерела) на нову (цільову),
і обидві послідовності можуть мати довільну довжину.
Приклади завдань трансформації включають машинний переклад
між кількома мовами як у тексті, так і в аудіо,
генерацію діалогу запитання-відповідь або навіть синтаксичний
розбір речень на дерева граматики.

Модель seq2seq зазвичай має архітектуру енкодер-декодер, що складається з:

\begin{enumerate}[label=--]
    \item Енкодер обробляє вхідну послідовність і стискає інформацію
    у контекстний вектор (також відомий як ембедінг речення
    або вектор ``думки'') фіксованої довжини. Очікується,
    що це відображення буде гарним підсумком значення
    цілої послідовності.
    \item Декодер ініціалізується контекстним вектором для
    виводу перетвореної послідовності. Ранні роботи
    використовували лише останній стан мережі
    енкодера як початковий стан декодера.
\end{enumerate}

І енкодер, і декодер є рекурентними нейронними мережами.

Критичним і очевидним недоліком цього дизайну контексту з
фіксованою довжиною є нездатність запам'ятовувати довгі речення.
Часто він забуває першу частину, як тільки завершує обробку всього
вводу. Для вирішення цієї проблеми народився
механізм уваги \cite{nn:attention}.

\section{Механізм уваги}
Увага до певної міри мотивована тим, як ми приділяємо зорову увагу
різним регіонам зображення або співвідносимо слова в одному реченні.

Зорова увага людини дозволяє зосередитись на певному регіоні з
``високою роздільною здатністю'', одночасно сприймаючи навколишнє
зображення в ``низькій роздільній здатності'', а потім відрегувати
фокусну точку або зробити відповідний висновок.
Враховуючи невеликий фрагмент зображення, пікселі в решті надають підказки, що
там відображається. Ми очікуємо побачити загострене вухо в жовтій коробці,
тому що ми бачили ніс собаки, ще одне загострене вухо праворуч і очі
Шіби (речі в червоних квадратах) (Рис. \ref{fig:shiba}). Однак
светр та ковдра внизу були
б не такими корисними, як ті собачі риси.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{shiba-example-attention.png}
    \caption{Шіба-іну в одязі}
    \label{fig:shiba}
\end{figure}

Подібним чином ми можемо пояснити зв'язок між словами в одному реченні
або близькому контексті. Коли ми бачимо ``їсти'', ми сподіваємось дуже
скоро зустріти слово, що означає вид їжи.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{sentence-example-attention.png}
    \caption{Одне слово ``поділяє увагу'' іншим словам по-різному}
    \label{fig:attend-example}
\end{figure}

Увагу при глибокому навчанні можна широко трактувати як вектор вагових
коефіцієнтів: для того, щоб передбачити або зробити висновок про один елемент,
наприклад, піксель на зображенні або слово в реченні, ми оцінюємо,
використовуючи вектор уваги, наскільки сильно він співвідноситься з іншими
елементами,
і приймає суму їх значень, зважену вектором уваги, як наближення
цілі \cite{attention}.

Механізм уваги народився, щоб допомогти запам’ятати довгі початкові
речення в машинному нейронному перекладі (МНП). Замість того,
щоб будувати єдиний вектор контексту із останнього прихованого стану
енкодера, підхід, винайдений увагою, полягає у створенні
скорочених шляхів між контекстним вектором та
всією вихідною послідовністю. Ваги цих з'єднань скорочення
можна підправити для кожного вихідного елемента.

Хоча вектор контексту має доступ до всієї початкової послідовності,
нам не потрібно турбуватися про забування.
Відповідність між джерелом та ціллю вивчається та контролюється
за допомогою контекстного вектора. По суті, вектор контексту
споживає три частини інформації:

\begin{enumerate}[label=--]
    \item прихований стан енкодеру;
    \item прихований стан декодеру;
    \item відповідність між входом та ціллю.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{encoder-decoder-attention.png}
    \caption{Архітектура рекурентної моделі з адитивною увагою}
    \label{fig:plot3}
\end{figure}

Механізм можна формально визначити так. Нехай є початкова послідовність
$x$ довжини $n$, і треба вивести цільову послідовність $y$
довжини $m$:

\vspace{0.5em}
\begin{gather}
\begin{aligned}
    x &= [x_1,x_2, ..., x_n] \\
    y &= [y_1,y_2, ..., y_n]
\end{aligned}
\end{gather}
\vspace{\baselineskip}

Тут енкодер -- це двонаправлена РНМ (або будь-яка інша РНМ) із прямим
прихованим станом $\overrightarrow{h_i}$ та зворотнім
$\overleftarrow{h_i}$. Звичайна конкатенація двох представляє стан
енкодера. Мотивація цієї операції
полягає в тому, щоб включити як попередні, так і наступні
слова до анотації одного слова.

\begin{equation}
    h_i = [\overrightarrow{h_i}^T;\overleftarrow{h_i}^T], i=1, ..., n
\end{equation}

Мережа декодера має прихований стан $s_t = f(s_{t-1},t_{t-1},c_t)$
для слова, що виводиться, на позиції $t$, $t = 1, ..., m$,
де контекстний вектор $c_t$ є сумою зважених прихованих станів
вхідної послідовності:

\vspace{0.5em}
\begin{gather}
\begin{aligned}
    c_t          &= \sum^n_{i=1}\alpha_{t,i} h_i \\
    \alpha_{t,i} &= \text{align}(y_t,x_t) \\
    &= \frac{\exp(\text{score}(s_{t-1}, h_i))}{\sum^n_{i'=1}\exp(\text{score}(s_{t-1},h_{i'}))}
\end{aligned}
\end{gather}
\vspace{\baselineskip}

Модель відповідності призначає оцінку $\alpha_{t,i}$
парі вхідних даних у позиції $i$ та виході у позиції $t$, ($y_t$, $x_i$),
залежно від того, наскільки вони співвідносяться. Множина
${α_{t,i}}$ -- це ваги, що визначають, наскільки кожен прихований стан
входу слід враховувати для кожного виводу.
У статті Багданау \cite{nn:attention} оцінка відповідності
$α$ параметризується мережею прямого пересилання
з єдиним прихованим шаром, і ця мережа навчається спільно з
іншими частинами моделі. Таким чином, функція оцінки
має такий вигляд, враховуючи, що $\tanh$ використовується як
нелінійна функція активації:

\begin{equation}
    \text{score}(s_t, h_i) = v^T_a \tanh(W_a [s_t;h_i])
\end{equation}
Де обидва $v_a$ та $W_a$ є матрицями вагів, які вивчаються
моделлю відповідності.

Існує декілька варіацій механізму
уваги \cite{attention:1} \cite{attention:2}, які змінюють
функцію вираховування відповідності. Вище
була використана аддитивна \cite{nn:attention} увага.

\section{Трансформери}
Робота ``Attention Is All You Need'' \cite{attention-all-need}
ввела архітектуру
``Трансформера'', яка була використана для задачі машинного
перекладу. Модель уникає необхідності використовувати
рекурентні з'єднання, та цілком полагається на
механізм уваги.

Основним компонентом трансформера є блок
багатоголовочної уваги. Трансформер розглядає закодоване
представлення входу як набір пар ключ-значення $(K, V)$,
обидва розмірності $n$ (довжина вхідної послідовності).
У декодері попередній вивід стискається у запит 
($Q$ розмірності $m$), а наступний вивід створюється
шляхом відображення цього запиту та набору ключів і значень.

Трансформер використовує масштабовану увагу
скалярного добутку: вихід є зваженою сумою значень,
де вага, що присвоєна кожному значенню, визначається
скалярним добутком запиту з усіма ключами:

\begin{equation}
    \text{Attention(Q,K,V)} = \text{softmax}(\frac{QK^T}{\sqrt{n}})V
\end{equation}

Де
\begin{equation}
    Q=XW_Q, K=XW_K, V=XW_V
\end{equation}

Тут $XW_Q, XW_K, XW_V$ це матриці ваг, які вивчаються
моделлю. $n$ це розмір матриці, який потрібен
для зменшення проблени зникаючих градієнтів \cite{attention-all-need}.

Важлимим компонентом трансформера є те, що
механізм самоуваги використовується декілька разів.
Замість того, щоб обчислити увагу лише один раз,
багатоголочна увага проходить через масштабовану увагу
скалярного добутку кілька разів паралельно. Виходи головок
уваги просто конкатенуються та лінійно перетворюються
до очікуваних розмірів. Це дозволяє різним голівкам
навчатися приділяти увагу різним речам.

\vspace{0.5em}
\begin{gather}
\begin{aligned}
    \text{MultiHead}(Q,K,V)= [\text{head}_1; ...; \text{head}_h]W^O \\
    \text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\end{aligned}
\end{gather}
\vspace{\baselineskip}

Де $QW^Q_i$, $KW^K_i$, $VW^V_i$ та $W^O$ -- матриці вагів,
які модель вивчає під час тренування.
Множення конкатенованого вектору на матрицю $W^O$
дозволяє накладати шари багатоголовочної уваги друг на друга,
тим самим збільшуючи силу представлення моделі, та дозволяючи
їй вивчати більш складні асоціації між словами.
Ілюстрацію багатоголовочного механізму уваги можна
побачити на рисунку \ref{fig:trans-multihead}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{multi-head-attention.png}
    \caption{Ілюстрація багатоголовочного механізму уваги}
    \label{fig:trans-multihead}
\end{figure}

Архітектура трансформера зі статті ``Attention Is All You Need''
використовувалась для задачі машинного перекладу, де
стандартна архітектура складається з енкодера, та декодера.
Енкодер створює представлення вхідного
потенційно-безкінечного контексту
на основі механізму уваги, а декодер витягує закодовану інформацію,
та виконує поточне завдання. Повна архітектура трансформера
є на рисунку \ref{fig:trans-arch}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{transformer.png}
    \caption{Повна архітектура моделі трансформера}
    \label{fig:trans-arch}
\end{figure}

Таким чином, архітектури, засновані на трансформерах
уникають використання рекурентності у нейронних мережах і замість
цього цілком довіряють механізмам самоуваги для
вирішення задач, які традиційно найкраще вирішували
різні варіації рекурентних мереж.

\section{Постановка задачі}
Архітектура трансформер стала фактичним стандартом
для завдань з обробки природної мови, проте її застосування
до комп'ютерного зору залишається обмеженим. У комп'ютерному
зорі увага або застосовується разом із згортковими мережами (ЗНМ),
або використовується для заміни певних компонентів
згорткових мереж, зберігаючи загальну їх структуру на місці.

Задача цієї роботи полягає у застосуванні архітектури
трансформеру до задачі класифікації зображень. Ми хочемо показати,
що залежність від згорткових мереж не є необхідною, і чистий
трансформер, що застосовується безпосередньо до послідовностей
клаптиків зображень, може дуже добре виконувати
завдання класифікації зображень.

Основна стаття \cite{image-worth-16-16-words},
на якій основана ця робота, полягає у використання
трансформера до задачі з іншого домену, при умові що
даних та обчислювальних ресурсів достатньо.
Так як ми не маємо таких обчислювальних
можливастей, як автори основної статті, то ми беремо
експериментальні дослідження з оригіналу \cite{image-worth-16-16-words}.
