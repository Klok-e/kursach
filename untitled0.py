# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14fC_9iA_YeoHZ8bBP82unWHQAv9_EWFc
"""

!pip install einops

import time
import torch
import torchvision
import torchvision.transforms as transforms
from torch import nn, einsum
import torch.nn.functional as F
from torch import optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torch.nn.init as init

from einops import rearrange, repeat
from einops.layers.torch import Rearrange

# credit for the transformer implementation to https://github.com/lucidrains/vit-pytorch
# helpers
def pair(t):
    return t if isinstance(t, tuple) else (t, t)

# classes
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        b, n, _, h = *x.shape, self.heads
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)

        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale

        attn = self.attend(dots)

        out = einsum('b h i j, b h j d -> b h i d', attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x

class ViT(nn.Module):
    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
        super().__init__()
        image_height, image_width = pair(image_size)
        patch_height, patch_width = pair(patch_size)

        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        patch_dim = channels * patch_height * patch_width
        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'

        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
            nn.Linear(patch_dim, dim),
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.dropout = nn.Dropout(emb_dropout)

        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)

        self.pool = pool
        self.to_latent = nn.Identity()

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )

    def forward(self, img):
        x = self.to_patch_embedding(img)
        b, n, _ = x.shape

        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding[:, :(n + 1)]
        x = self.dropout(x)

        x = self.transformer(x)

        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]

        x = self.to_latent(x)
        return self.mlp_head(x)

device = torch.device("cuda:0")
#device = torch.device("cpu")
CLASSES=100

vit = ViT(
    image_size = 32,
    patch_size = 8,
    num_classes = CLASSES,
    dim = 128,
    depth = 6,
    heads = 14,
    mlp_dim = 512,
    dropout = 0.1,
    emb_dropout = 0.1
)
vit.to(device)

# https://github.com/akamaster/pytorch_resnet_cifar10/
def _weights_init(m):
    classname = m.__class__.__name__
    #print(classname)
    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
        init.kaiming_normal_(m.weight)

class LambdaLayer(nn.Module):
    def __init__(self, lambd):
        super(LambdaLayer, self).__init__()
        self.lambd = lambd

    def forward(self, x):
        return self.lambd(x)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, in_planes, planes, stride=1, option='A'):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != planes:
            if option == 'A':
                """
                For CIFAR10 ResNet paper uses option A.
                """
                self.shortcut = LambdaLayer(lambda x:
                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), "constant", 0))
            elif option == 'B':
                self.shortcut = nn.Sequential(
                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                     nn.BatchNorm2d(self.expansion * planes)
                )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out


class ResNet(nn.Module):
    def __init__(self, block, num_blocks, num_classes=10):
        super(ResNet, self).__init__()
        self.in_planes = 16

        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)
        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)
        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)
        self.linear = nn.Linear(64, num_classes)

        self.apply(_weights_init)

    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion

        return nn.Sequential(*layers)

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = F.avg_pool2d(out, out.size()[3])
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out

net = ResNet(BasicBlock, [5, 5, 5], CLASSES)
net.to(device)

# load cifar 10
transform = transforms.Compose(
    [transforms.ToTensor()])

batch_size = 1024

trainset = torchvision.datasets.CIFAR100(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True)

testset = torchvision.datasets.CIFAR100(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False)

next(iter(trainloader))[0].shape

trainset

testset

def count_params(net):
  return sum(p.numel() for p in net.parameters())

net_accs=[]
vit_accs=[]

net_losses=[]
vit_losses=[]

def test(network, dataset_loader,criterion,prefix,accuracy_list):
  network.eval()
  test_loss = 0
  correct = 0
  total = 0
  with torch.no_grad():
    for data, label in dataset_loader:
      data = data.to(device)
      label = label.to(device)
      output = network(data)
      test_loss += criterion(output, label).item()
      _, predicted = torch.max(output.data, 1)
      total += label.size(0)
      correct += (predicted == label).sum().item()
  test_loss /= total
  acc = 100. * correct / total
  print(f"{prefix} Loss: {test_loss:.4f}; Accuracy: {acc:.4f}%")
  accuracy_list.append(acc)
  return test_loss

def epoch_network(network, criterion, optimizer,losses_list,accuracies_list):
  running_loss = 0.0
  for i, data in enumerate(trainloader):
    network.train()
    # get the inputs; data is a list of [inputs, labels]
    inputs, labels = data
    inputs = inputs.to(device)
    labels = labels.to(device)

    # zero the parameter gradients
    optimizer.zero_grad()

    # forward + backward + optimize
    outputs = network(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    # print statistics
    running_loss += loss.item()
    if i % log_frequency == log_frequency - 1:
      print(f'Epoch: {epoch}, item: {i * batch_size} loss: {(running_loss / log_frequency):.4f}')
      losses_list.append(running_loss / log_frequency)
      running_loss = 0.0

      test(network,testloader,criterion,"Test",accuracies_list)

epochs = 50
log_frequency = 4096//batch_size
lr = 0.001

criterion = nn.CrossEntropyLoss()
net_optimizer = optim.Adam(net.parameters(), lr=lr)
net_scheduler = ReduceLROnPlateau(net_optimizer, patience=1)

vit_optimizer = optim.Adam(vit.parameters(), lr=lr)
vit_scheduler = ReduceLROnPlateau(vit_optimizer, patience=1)

print(f"CNN number of params: {count_params(net)}")
print(f"ViT number of params: {count_params(vit)}")

test(net,testloader,criterion,"CNN Test untrained",net_accs)
test(vit,testloader,criterion,"ViT Test untrained",vit_accs)
for epoch in range(epochs):  # loop over the dataset multiple times
  print("\nCNN")
  t1=time.time()
  epoch_network(net,criterion,net_optimizer,net_losses,net_accs)
  net_loss=test(net,testloader,criterion,"Test",net_accs)
  net_scheduler.step(net_loss)
  print(f"learning rate: {net_optimizer.param_groups[0]['lr']}")
  print(f"CNN elapsed since epoch start: {time.time() - t1}")

  print("\nViT")
  t1=time.time()
  epoch_network(vit,criterion,vit_optimizer,vit_losses,vit_accs)
  vit_loss=test(vit,testloader,criterion,"Test",vit_accs)
  vit_scheduler.step(vit_loss)
  print(f"learning rate: {vit_optimizer.param_groups[0]['lr']}")
  print(f"ViT elapsed since epoch start: {time.time() - t1}")

  torch.save(net.state_dict(), 'net_model.pth')
  torch.save(net_optimizer.state_dict(), 'net_optimizer.pth')

  torch.save(vit.state_dict(), 'vit_model.pth')
  torch.save(vit_optimizer.state_dict(), 'vit_optimizer.pth')

print('Finished Training')

# 64 epochs
min_len_acc=min(len(net_accs),len(vit_accs))
net_accs = net_accs[:min_len_acc]
vit_accs = vit_accs[:min_len_acc]
print(net_accs)
print(vit_accs)

min_len_loss=min(len(net_losses),len(vit_losses))
net_losses = net_losses[:min_len_loss]
vit_losses = vit_losses[:min_len_loss]
print(net_losses)
print(vit_losses)

from matplotlib import pyplot as plt

# accuracy
y = vit_accs
x = list(map(lambda x: x[0],enumerate(y)))
plt.subplot(1,2,1)
plt.plot(x,y, "b^-", label="ViT",markevery=70)

y = net_accs
x = list(map(lambda x: x[0],enumerate(y)))
plt.plot(x,y, "go-", label="ResNet32",markevery=70)
plt.ylabel('Accuracy')
plt.legend()

# loss
y = vit_losses
x = list(map(lambda x: x[0],enumerate(y)))
plt.subplot(1,2,2)
plt.plot(x,y, "b^-",label="ViT",markevery=70)

y = net_losses
x = list(map(lambda x: x[0],enumerate(y)))

plt.plot(x,y,"go-", label="ResNet32",markevery=70)
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig("plot.png",dpi=150)