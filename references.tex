\addcontentsline{toc}{chapter}{Перелік джерел посилання}
\renewcommand\bibname{ПЕРЕЛІК ДЖЕРЕЛ ПОСИЛАННЯ}
\begin{thebibliography}{9}
    \bibitem{cortex-stuff}
    Hubel D. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex / D. Hubel, T. Wiesel. // The Journal of physiology. – 1962. – С. 106–154.

    \bibitem{cortex-equations}
    Hodgkin A. quantitative description of membrane current and its application to conduction and excitation in nerve / A. Hodgkin, A. Huxley. // The Journal of physiology. – 1952. – С. 500–544.

    \bibitem{rozenblatt}
    Rosenblatt F. The perceptron, a perceiving and recognizing automaton Project Para / Frank Rosenblatt. // Cornell Aeronautical Laboratory. – 1957.

    \bibitem{nn:peyre}
    Peyré G. Mathematics of Neural Networks [Електронний ресурс] / Gabriel Peyré – Режим доступу до ресурсу: https://mathematical-tours.github.io/book-basics-sources/neural-networks-en/NeuralNetworksEN.pdf.

    \bibitem{nn:multilayer-perceptrons}
    Grosse R. Lecture 5: Multilayer Perceptrons [Електронний ресурс] / Roger Grosse – Режим доступу до ресурсу: https://www.cs.toronto.edu/~mren/teach/csc411\_19s/lec/lec10\_notes1.pdf.

    \bibitem{nn:backpropagation}
    Rumelhart D. Learning representations by back-propagating errors / D. Rumelhart, G. Hinton, R. Williams. // nature. – 1986. – С. 533–536.

    \bibitem{gradient-descend}
    Ruder S. An overview of gradient descent optimization algorithms / Sebastian Ruder. // arXiv preprint arXiv:1609.04747. – 2016.

    \bibitem{nn:recurrent-hard}
    Bengio Y. Learning long-term dependencies with gradient descent is difficult / Y. Bengio, P. Simard, P. Frasconi. // IEEE Transactions on Neural Networks. – 1994. – С. 157–166.

    \bibitem{nn:lstm}
    Hochreiter S. Long Short-term Memory / S. Hochreiter, J. Schmidhuber. // Neural computation. – 1997. – С. 1735–80.

    \bibitem{nn:gru}
    Cho K. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches / K. Cho, B. van Merri ̈enboer, D. Bahdanau. // arXiv preprint arXiv:1409.1259. – 2014.

    \bibitem{nn:seq2seq}
    Sutskever I. Sequence to Sequence Learningwith Neural Networks / I. Sutskever, O. Vinyals, Q. V. Le. // arXiv:1409.3215. – 2014.

    \bibitem{nn:attention}
    Bahdanau D. Neural Machine Translation by Jointly Learning to Align and Translate / D. Bahdanau, K. Cho, Y. Bengio. // arXiv:1409.0473. – 2014.

    \bibitem{attention}
    Weng L. Attention? Attention! [Електронний ресурс] / Lilian Weng // lilianweng.github.io/lil-log. – 2018. – Режим доступу до ресурсу: http://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html.

    \bibitem{attention:1}
    Graves A. Neural Turing Machines / A. Graves, G. Wayne, I. Danihelka. // arXiv:1410.5401. – 2014.

    \bibitem{attention:2}
    Luong M. Effective Approaches to Attention-based Neural Machine Translation / M. Luong, H. Pham, C. D. Manning. // arXiv:1508.04025. – 2015.

    \bibitem{attention-all-need}
    Attention is All You Need / [A. Vaswani, N. Shazeer, N. Parmar та ін.]. // arXiv:1706.03762. – 2017.

    \bibitem{image-worth-16-16-words}
    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale / [A. Dosovitskiy, L. Beyer, A. Kolesnikov та ін.]. // CoRR. – 2020.

    \bibitem{bert}
    BERT: Pre-training of deep bidirectional transformers for language understanding / J.Devlin, M. Chang, K. Lee, K. Toutanova. // NAACL. – 2019.

    \bibitem{gpt}
    Improving language understanding with unsupervised learning / [A. Radford, K. Narasimhan, T. Salimans та ін.]. // Technical Report. – 2018.

    \bibitem{nn:cnn-rnn}
    CNN-RNN: A Unified Framework for Multi-label Image Classification / [J. Wang, Y. Yang, J. Mao та ін.]. // arXiv:1604.04573. – 2016. – С. 2285–2294.

    \bibitem{image-trans}
    Image transformer / [N. Parmar, A. Vaswani, J. Uszkoreit та ін.]. // ICML. – 2018.

    \bibitem{local-regions-attention}
    Hu H. Local relation networks for image recognition / H. Hu, Z. Zhang, Z. Xie. // ICCV. – 2019.

    \bibitem{cordonnier}
    Cordonnier J. On the relationship between self-attention and convolutional layers / J. Cordonnier, A. Loukas, M. Jaggi. // ICLR. – 2020.

    \bibitem{gelu}
    Hendrycks D. Gaussian Error Linear Units (GELUs) / D. Hendrycks, K. Gimpel. // CoRR. – 2016.

    \bibitem{big-transfer}
    Big transfer (BiT): General visual representation learning / [A. Kolesnikov, L. Beyer, X. Zhai та ін.]. // ECCV. – 2020.

\end{thebibliography}